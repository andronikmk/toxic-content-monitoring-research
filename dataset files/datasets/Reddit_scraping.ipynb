{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Current_Project\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    " \n",
    "import squarify\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is a tool for scraping text data from self posts on multiple subreddits, \n",
    "# then tokeinzing and lemmatizing the text post \n",
    "\n",
    "# First, a scraper\n",
    "# client_id and client_secret are unique to each reddit user. Use yours before running this notebook.\n",
    "reddit = praw.Reddit(client_id='BuYtI7QK33-lng', client_secret='Cyu6wGzW2pBsFyVWRShY8vEaUsg', user_agent='Scraping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_list = ['suicidewatch', 'casualconversation', 'self', 'relationship_advice']\n",
    "\n",
    "posts = []\n",
    "\n",
    "for subs in subs_list:\n",
    "    sw_subreddit = reddit.subreddit(subs)\n",
    "    for post in sw_subreddit.top(limit = 1000):\n",
    "        posts.append([post.title, post.score, post.id, post.subreddit, post.num_comments, post.selftext])\n",
    "\n",
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'num_comments', 'body']).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then a tokenizer/lemmatizer\n",
    "\n",
    "Tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(docs):\n",
    "\n",
    "        word_counts = Counter()\n",
    "        appears_in = Counter()\n",
    "        \n",
    "        total_docs = len(docs)\n",
    "\n",
    "        for doc in docs:\n",
    "            word_counts.update(doc)\n",
    "            appears_in.update(set(doc))\n",
    "\n",
    "        temp = zip(word_counts.keys(), word_counts.values())\n",
    "        \n",
    "        wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
    "\n",
    "        wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
    "        total = wc['count'].sum()\n",
    "\n",
    "        wc['pct_total'] = wc['count'].apply(lambda x: x / total)\n",
    "        \n",
    "        wc = wc.sort_values(by='rank')\n",
    "        wc['cul_pct_total'] = wc['pct_total'].cumsum()\n",
    "\n",
    "        t2 = zip(appears_in.keys(), appears_in.values())\n",
    "        ac = pd.DataFrame(t2, columns=['word', 'appears_in'])\n",
    "        wc = ac.merge(wc, on='word')\n",
    "\n",
    "        wc['appears_in_pct'] = wc['appears_in'].apply(lambda x: x / total_docs)\n",
    "        \n",
    "        return wc.sort_values(by='rank')\n",
    "\n",
    "def get_lemmas(text):\n",
    "\n",
    "    lemmas = []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Something goes here :P\n",
    "    for token in doc: \n",
    "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_!= 'PRON'):\n",
    "            lemmas.append(token.lemma_)\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "printable = set(string.printable)\n",
    "\n",
    "def cleanup(x):\n",
    "    x = \" \".join(x.split(\"\\\\n\"))\n",
    "    x = \" \".join(x.split(\"\\\\t\"))\n",
    "    x = \" \".join(x.split(\"\\\\r\"))\n",
    "    x = \" \".join(x.split(\"\\n\"))\n",
    "    x = \" \".join(x.split(\"\\t\"))\n",
    "    x = \" \".join(x.split(\"\\r\"))\n",
    "    x = \" \".join(x.split(\",\"))\n",
    "    x = \" \".join(x.split(\".\"))\n",
    "    x = \" \".join(x.split(\"!\"))\n",
    "    x = \" \".join(x.split(\"?\"))\n",
    "    x = x.lower()\n",
    "    x = \"\".join(list(filter(lambda c: c in printable, x)))\n",
    "    x = \" \".join(filter(lambda z: z != '', x.split(\" \")))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>Contemplating suicide due to being a 50 year o...</td>\n",
       "      <td>165</td>\n",
       "      <td>9k0d4y</td>\n",
       "      <td>True</td>\n",
       "      <td>89</td>\n",
       "      <td>I'm so fucking sick and tired of this life. I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Just got told to kill myself</td>\n",
       "      <td>324</td>\n",
       "      <td>e1wfoz</td>\n",
       "      <td>True</td>\n",
       "      <td>66</td>\n",
       "      <td>I guess that’s it.        \\n   \\nI didn’t expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>I wonder how many tourist's photos I'm in. I w...</td>\n",
       "      <td>4279</td>\n",
       "      <td>aaauag</td>\n",
       "      <td>False</td>\n",
       "      <td>276</td>\n",
       "      <td>I live in St. Augustine and it is quite a tour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>I remember one time when I was in like 1st gra...</td>\n",
       "      <td>19466</td>\n",
       "      <td>e0feat</td>\n",
       "      <td>False</td>\n",
       "      <td>354</td>\n",
       "      <td>And he said, “Yeah, that makes sense. You prob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>Hey reddit... my wife just stayed up with me u...</td>\n",
       "      <td>725</td>\n",
       "      <td>envjq</td>\n",
       "      <td>False</td>\n",
       "      <td>250</td>\n",
       "      <td>Only you guys could truly appreciate what an a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  score      id  \\\n",
       "837   Contemplating suicide due to being a 50 year o...    165  9k0d4y   \n",
       "442                        Just got told to kill myself    324  e1wfoz   \n",
       "1600  I wonder how many tourist's photos I'm in. I w...   4279  aaauag   \n",
       "1019  I remember one time when I was in like 1st gra...  19466  e0feat   \n",
       "2833  Hey reddit... my wife just stayed up with me u...    725   envjq   \n",
       "\n",
       "      subreddit  num_comments  \\\n",
       "837        True            89   \n",
       "442        True            66   \n",
       "1600      False           276   \n",
       "1019      False           354   \n",
       "2833      False           250   \n",
       "\n",
       "                                                   body  \n",
       "837   I'm so fucking sick and tired of this life. I ...  \n",
       "442   I guess that’s it.        \\n   \\nI didn’t expe...  \n",
       "1600  I live in St. Augustine and it is quite a tour...  \n",
       "1019  And he said, “Yeah, that makes sense. You prob...  \n",
       "2833  Only you guys could truly appreciate what an a...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time 0.2465674877166748\n"
     ]
    }
   ],
   "source": [
    "# Finally, some cleaning up of dirty text.\n",
    "\n",
    "import time\n",
    "import re\n",
    "\"\"\"Lemmatizing and stemming gives us a lower ROC-AUC score. So we will only clean \\\\n's, Username, IP and http links\"\"\"\n",
    "\n",
    "start_time=time.time()\n",
    "# remove '\\\\n'\n",
    "posts['body'] = posts['body'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "    \n",
    "# remove any text starting with User... \n",
    "posts['body'] = posts['body'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
    "    \n",
    "# remove IP addresses or user IDs\n",
    "posts['body'] = posts['body'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "    \n",
    "#remove http links in the text\n",
    "posts['body'] = posts['body'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n",
    "\n",
    "end_time=time.time()\n",
    "print(\"total time\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 649 ms\n"
     ]
    }
   ],
   "source": [
    "%time posts['body'] = posts['body'].apply(cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%time posts['lemmas'] = posts['body'].apply(get_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>Contemplating suicide due to being a 50 year o...</td>\n",
       "      <td>165</td>\n",
       "      <td>9k0d4y</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>i'm so fucking sick and tired of this life i j...</td>\n",
       "      <td>[fuck, sick, tired, life, turn, 50, month, hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Just got told to kill myself</td>\n",
       "      <td>324</td>\n",
       "      <td>e1wfoz</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>i guess thats it i didnt expect this to blow u...</td>\n",
       "      <td>[guess, s, not, expect, blow, like, thank, lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>I wonder how many tourist's photos I'm in. I w...</td>\n",
       "      <td>4279</td>\n",
       "      <td>aaauag</td>\n",
       "      <td>0</td>\n",
       "      <td>276</td>\n",
       "      <td>i live in st augustine and it is quite a touri...</td>\n",
       "      <td>[live, st, augustine, tourist, attraction, bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>I remember one time when I was in like 1st gra...</td>\n",
       "      <td>19466</td>\n",
       "      <td>e0feat</td>\n",
       "      <td>0</td>\n",
       "      <td>354</td>\n",
       "      <td>and he said yeah that makes sense you probably...</td>\n",
       "      <td>[say, yeah, make, sense, probably, not, want, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>Hey reddit... my wife just stayed up with me u...</td>\n",
       "      <td>725</td>\n",
       "      <td>envjq</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>only you guys could truly appreciate what an a...</td>\n",
       "      <td>[guy, truly, appreciate, amazing, love, big, c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  score      id  \\\n",
       "837   Contemplating suicide due to being a 50 year o...    165  9k0d4y   \n",
       "442                        Just got told to kill myself    324  e1wfoz   \n",
       "1600  I wonder how many tourist's photos I'm in. I w...   4279  aaauag   \n",
       "1019  I remember one time when I was in like 1st gra...  19466  e0feat   \n",
       "2833  Hey reddit... my wife just stayed up with me u...    725   envjq   \n",
       "\n",
       "      subreddit  num_comments  \\\n",
       "837           1            89   \n",
       "442           1            66   \n",
       "1600          0           276   \n",
       "1019          0           354   \n",
       "2833          0           250   \n",
       "\n",
       "                                                   body  \\\n",
       "837   i'm so fucking sick and tired of this life i j...   \n",
       "442   i guess thats it i didnt expect this to blow u...   \n",
       "1600  i live in st augustine and it is quite a touri...   \n",
       "1019  and he said yeah that makes sense you probably...   \n",
       "2833  only you guys could truly appreciate what an a...   \n",
       "\n",
       "                                                 lemmas  \n",
       "837   [fuck, sick, tired, life, turn, 50, month, hav...  \n",
       "442   [guess, s, not, expect, blow, like, thank, lea...  \n",
       "1600  [live, st, augustine, tourist, attraction, bea...  \n",
       "1019  [say, yeah, make, sense, probably, not, want, ...  \n",
       "2833  [guy, truly, appreciate, amazing, love, big, c...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from symspellpy import SymSpell\n",
    "\n",
    "corpus = []\n",
    "for line in posts['lemmas'].values:\n",
    "    tokens = [token for token in line if len(token) > 0]\n",
    "    corpus.extend(tokens)\n",
    "    \n",
    "with open('toxicCorpus.txt', 'w') as filehandle:\n",
    "        for listitem in corpus:\n",
    "            filehandle.write('%s\\n' % listitem)\n",
    "\n",
    "symspell = SymSpell()\n",
    "symspell.create_dictionary(corpus=\"toxicCorpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctSpelling(x):\n",
    "    corr = symspell.lookup(x, verbosity=10)\n",
    "    if len(corr) > 0:\n",
    "        return corr[0].term\n",
    "    \n",
    "    return x\n",
    "\n",
    "posts['lemmas'] = [ [ correctSpelling(lemma) for lemma in line]\n",
    "                 for line in posts['lemmas'].values ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = posts['lemmas']\n",
    "y = posts['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.to_csv('reddit_posts.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
