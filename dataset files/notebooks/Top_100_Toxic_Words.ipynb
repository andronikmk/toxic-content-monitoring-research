{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    " \n",
    "import squarify\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(docs):\n",
    "\n",
    "        word_counts = Counter()\n",
    "        appears_in = Counter()\n",
    "        \n",
    "        total_docs = len(docs)\n",
    "\n",
    "        for doc in docs:\n",
    "            word_counts.update(doc)\n",
    "            appears_in.update(set(doc))\n",
    "\n",
    "        temp = zip(word_counts.keys(), word_counts.values())\n",
    "        \n",
    "        wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
    "\n",
    "        wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
    "        total = wc['count'].sum()\n",
    "\n",
    "        wc['pct_total'] = wc['count'].apply(lambda x: x / total)\n",
    "        \n",
    "        wc = wc.sort_values(by='rank')\n",
    "        wc['cul_pct_total'] = wc['pct_total'].cumsum()\n",
    "\n",
    "        t2 = zip(appears_in.keys(), appears_in.values())\n",
    "        ac = pd.DataFrame(t2, columns=['word', 'appears_in'])\n",
    "        wc = ac.merge(wc, on='word')\n",
    "\n",
    "        wc['appears_in_pct'] = wc['appears_in'].apply(lambda x: x / total_docs)\n",
    "        \n",
    "        return wc.sort_values(by='rank')\n",
    "\n",
    "def get_lemmas(text):\n",
    "\n",
    "    lemmas = []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Something goes here :P\n",
    "    for token in doc: \n",
    "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_!= 'PRON'):\n",
    "            lemmas.append(token.lemma_)\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"toxic-train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "printable = set(string.printable)\n",
    "\n",
    "def cleanup(x):\n",
    "    x = \" \".join(x.split(\"\\\\n\"))\n",
    "    x = \" \".join(x.split(\"\\\\t\"))\n",
    "    x = \" \".join(x.split(\"\\\\r\"))\n",
    "    x = \" \".join(x.split(\"\\n\"))\n",
    "    x = \" \".join(x.split(\"\\t\"))\n",
    "    x = \" \".join(x.split(\"\\r\"))\n",
    "    x = \" \".join(x.split(\",\"))\n",
    "    x = \" \".join(x.split(\".\"))\n",
    "    x = \" \".join(x.split(\"!\"))\n",
    "    x = \" \".join(x.split(\"?\"))\n",
    "    x = x.lower()\n",
    "    x = \"\".join(list(filter(lambda c: c in printable, x)))\n",
    "    x = \" \".join(filter(lambda z: z != '', x.split(\" \")))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%time df['comment_text'] = df['comment_text'].apply(cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 28min 2s\n"
     ]
    }
   ],
   "source": [
    "%time df['lemmas'] = df['comment_text'].apply(get_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"toxic-train-w-lems.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from symspellpy import SymSpell\n",
    "\n",
    "corpus = []\n",
    "for line in df['lemmas'].values:\n",
    "    tokens = [token for token in line if len(token) > 0]\n",
    "    corpus.extend(tokens)\n",
    "    \n",
    "with open('toxicCorpus.txt', 'w') as filehandle:\n",
    "        for listitem in corpus:\n",
    "            filehandle.write('%s\\n' % listitem)\n",
    "\n",
    "symspell = SymSpell()\n",
    "symspell.create_dictionary(corpus=\"toxicCorpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctSpelling(x):\n",
    "    corr = symspell.lookup(x, verbosity=10)\n",
    "    if len(corr) > 0:\n",
    "        return corr[0].term\n",
    "    \n",
    "    return x\n",
    "\n",
    "df['lemmas'] = [ [ correctSpelling(lemma) for lemma in line]\n",
    "                 for line in df['lemmas'].values ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTop(df, par, n=100, min_occurances=10):\n",
    "    wcT = count(df[df[par] == 1]['lemmas'])\n",
    "    wcT = wcT[wcT['count'] >= min_occurances]\n",
    "    wcF = count(df[df[par] == 0]['lemmas'])\n",
    "    wcF = wcF[wcF['count'] >= min_occurances]\n",
    "\n",
    "    wc = pd.merge(wcT, wcF, how='inner', on='word', suffixes=('_true', '_false'))\n",
    "    wc['more_true'] = wc['pct_total_true'] - wc['pct_total_false']\n",
    "    \n",
    "    return wc.sort_values(by ='more_true', ascending=False)[['word', 'more_true']].head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pars:\n",
    "    tp = getTop(df, p)['word'].values\n",
    "    \n",
    "    with open(p+'WordFile.txt', 'w') as filehandle:\n",
    "        for listitem in tp:\n",
    "            filehandle.write('%s\\n' % cleanup(listitem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"toxic-train-w-lems.csv\")\n",
    "df['lemmas'] = df2['lemmas'].apply(eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
