{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from overrides import overrides\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "from allennlp.data.fields import TextField, MetadataField, ArrayField\n",
    "from allennlp.predictors.sentence_tagger import SentenceTaggerPredictor\n",
    "from allennlp.nn.util import get_text_field_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "        \n",
    "config = Config(\n",
    "    testing=True,\n",
    "    seed=1,\n",
    "    batch_size=64,\n",
    "    lr=3e-4,\n",
    "    epochs=2,\n",
    "    hidden_sz=64,\n",
    "    max_seq_len=100, # necessary to limit memory usage\n",
    "    max_vocab_size=100000,\n",
    ")\n",
    "\n",
    "class JigsawDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer=lambda x: x.split(),\n",
    "                 token_indexers=None,\n",
    "                 max_seq_len=config.max_seq_len):\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    " \n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens, id=None, labels=None):\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "         \n",
    "        id_field = MetadataField(id)\n",
    "        fields[\"id\"] = id_field\n",
    "         \n",
    "        if labels is None:\n",
    "            labels = np.zeros(len(label_cols))\n",
    "        label_field = ArrayField(array=labels)\n",
    "        fields[\"label\"] = label_field\n",
    " \n",
    "        return Instance(fields)\n",
    "     \n",
    "    @overrides\n",
    "    def _read(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if config.testing: df = df.head(1000)\n",
    "        for i, row in df.iterrows():\n",
    "            yield self.text_to_instance(\n",
    "                [Token(x) for x in self.tokenizer(row[\"comment_text\"])],\n",
    "                row[\"id\"], row[label_cols].values,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_indexer = PretrainedBertIndexer(\n",
    "    pretrained_model=\"bert-base-uncased\",\n",
    "    max_pieces=config.max_seq_len,\n",
    "    do_lowercase=True,\n",
    "    truncate_long_sequences=False, # Use sliding window for contexts\n",
    " )\n",
    " \n",
    "def tokenizer(s: str):\n",
    "    return token_indexer.wordpiece_tokenizer(s)[:config.max_seq_len - 2]\n",
    "\n",
    "reader = JigsawDatasetReader(\n",
    "    tokenizer=tokenizer,\n",
    "    token_indexers={\"tokens\": token_indexer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embedder = PretrainedBertEmbedder(\n",
    "         pretrained_model=\"bert-base-uncased\",\n",
    "         top_layer_only=True, # conserve memory\n",
    ")\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": bert_embedder},\n",
    "                                        # we'll be ignoring masks so we'll need to set this to True\n",
    "                                        allow_unmatched_keys = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_DIM = word_embeddings.get_output_dim()\n",
    " \n",
    "class BertSentencePooler(Seq2VecEncoder):\n",
    "    def forward(self, embs: torch.tensor, \n",
    "                mask: torch.tensor=None) -> torch.tensor:\n",
    "        # extract first token tensor\n",
    "        return embs[:, 0]\n",
    "     \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return BERT_DIM\n",
    "     \n",
    "encoder = BertSentencePooler(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(Model):\n",
    "    def __init__(self, word_embeddings,\n",
    "                 encoder,\n",
    "                 out_sz=len(label_cols)):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.projection = nn.Linear(self.encoder.get_output_dim(), out_sz)\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "         \n",
    "    def forward(self, tokens,\n",
    "                id, label):\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        state = self.encoder(embeddings, mask)\n",
    "        class_logits = self.projection(state)\n",
    "         \n",
    "        output = {\"class_logits\": class_logits, \"state\": state}\n",
    "        output[\"loss\"] = self.loss(class_logits, label)\n",
    " \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(word_embeddings, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = SentenceTaggerPredictor(model, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_logits': [0.4939308762550354,\n",
       "  0.18493486940860748,\n",
       "  -0.0856907069683075,\n",
       "  -0.16543801128864288,\n",
       "  0.03377683088183403,\n",
       "  0.3908044993877411],\n",
       " 'state': [0.08956055343151093,\n",
       "  -0.04516694322228432,\n",
       "  0.15095344185829163,\n",
       "  -0.4251112639904022,\n",
       "  -0.4735545516014099,\n",
       "  -0.7761418223381042,\n",
       "  0.14447686076164246,\n",
       "  0.6996430158615112,\n",
       "  0.2625007927417755,\n",
       "  -0.1823796182870865,\n",
       "  0.08206482231616974,\n",
       "  -0.0040482450276613235,\n",
       "  0.2658458352088928,\n",
       "  0.4016270637512207,\n",
       "  -0.20179183781147003,\n",
       "  -0.308272123336792,\n",
       "  -0.19338111579418182,\n",
       "  0.5061939358711243,\n",
       "  0.345022976398468,\n",
       "  -0.0024992097169160843,\n",
       "  -0.11763352155685425,\n",
       "  -0.15226660668849945,\n",
       "  0.11027456074953079,\n",
       "  -0.2619054615497589,\n",
       "  -0.02325110137462616,\n",
       "  0.08160116523504257,\n",
       "  0.22415059804916382,\n",
       "  -0.09971704334020615,\n",
       "  -0.06847239285707474,\n",
       "  0.30956393480300903,\n",
       "  0.04546726867556572,\n",
       "  0.02099713310599327,\n",
       "  -0.3198445439338684,\n",
       "  -0.1277340054512024,\n",
       "  0.2875863313674927,\n",
       "  -0.3489682078361511,\n",
       "  0.1493016630411148,\n",
       "  0.23903878033161163,\n",
       "  0.16822907328605652,\n",
       "  -0.03922360762953758,\n",
       "  -0.309721440076828,\n",
       "  0.00991937704384327,\n",
       "  0.08401022851467133,\n",
       "  0.23718014359474182,\n",
       "  0.1033003181219101,\n",
       "  -0.17582552134990692,\n",
       "  -2.949700355529785,\n",
       "  -0.03951623663306236,\n",
       "  -0.22165709733963013,\n",
       "  -0.08732113242149353,\n",
       "  0.35361436009407043,\n",
       "  -0.5322600603103638,\n",
       "  0.02381659299135208,\n",
       "  0.34671396017074585,\n",
       "  0.17209550738334656,\n",
       "  0.01664000190794468,\n",
       "  -0.28687214851379395,\n",
       "  0.7450809478759766,\n",
       "  0.40798434615135193,\n",
       "  0.43644678592681885,\n",
       "  -0.10026490688323975,\n",
       "  0.05077497661113739,\n",
       "  -0.03768383339047432,\n",
       "  0.0857653021812439,\n",
       "  0.16065827012062073,\n",
       "  -0.12624911963939667,\n",
       "  0.19167323410511017,\n",
       "  0.14087404310703278,\n",
       "  0.24660158157348633,\n",
       "  0.7719461917877197,\n",
       "  -0.6435949206352234,\n",
       "  -0.46746471524238586,\n",
       "  0.24497872591018677,\n",
       "  -0.2648102045059204,\n",
       "  0.14408214390277863,\n",
       "  0.17406544089317322,\n",
       "  0.10146525502204895,\n",
       "  0.10118504613637924,\n",
       "  -0.026346411556005478,\n",
       "  0.1608138233423233,\n",
       "  0.35388484597206116,\n",
       "  0.4167953133583069,\n",
       "  0.21894651651382446,\n",
       "  0.0571318119764328,\n",
       "  0.24866649508476257,\n",
       "  0.2936916947364807,\n",
       "  -0.5032998323440552,\n",
       "  -0.5776887536048889,\n",
       "  0.22975704073905945,\n",
       "  0.5242965221405029,\n",
       "  -0.3830679953098297,\n",
       "  0.13795602321624756,\n",
       "  -0.12987801432609558,\n",
       "  0.40758052468299866,\n",
       "  0.37846243381500244,\n",
       "  0.18534837663173676,\n",
       "  -0.24988482892513275,\n",
       "  -0.12643779814243317,\n",
       "  0.17453277111053467,\n",
       "  0.44835084676742554,\n",
       "  0.3440724015235901,\n",
       "  -0.0816463977098465,\n",
       "  -0.10979567468166351,\n",
       "  -0.3957202434539795,\n",
       "  0.0181877538561821,\n",
       "  -0.1115703284740448,\n",
       "  0.016743524000048637,\n",
       "  -0.5787041187286377,\n",
       "  -0.11852753162384033,\n",
       "  -0.12113762646913528,\n",
       "  0.2969619035720825,\n",
       "  0.08746889233589172,\n",
       "  -0.26941949129104614,\n",
       "  0.3148200809955597,\n",
       "  -0.3971855342388153,\n",
       "  0.4892213046550751,\n",
       "  0.6070982813835144,\n",
       "  -0.06735044717788696,\n",
       "  0.6530959010124207,\n",
       "  0.3153579533100128,\n",
       "  -0.07587435096502304,\n",
       "  0.24062034487724304,\n",
       "  -0.1065162941813469,\n",
       "  -0.21773594617843628,\n",
       "  0.26306307315826416,\n",
       "  0.6592035889625549,\n",
       "  -0.031128376722335815,\n",
       "  -0.013977373018860817,\n",
       "  0.10096974670886993,\n",
       "  -0.1344054490327835,\n",
       "  0.1979910135269165,\n",
       "  0.7611749768257141,\n",
       "  0.00971737690269947,\n",
       "  -0.3895757496356964,\n",
       "  0.10782063752412796,\n",
       "  -0.025078527629375458,\n",
       "  -0.07799749821424484,\n",
       "  -0.07898546010255814,\n",
       "  -0.1044875830411911,\n",
       "  -0.40353959798812866,\n",
       "  -0.27961426973342896,\n",
       "  -0.0849556177854538,\n",
       "  -3.5558905601501465,\n",
       "  0.24793362617492676,\n",
       "  0.1454634815454483,\n",
       "  0.048436813056468964,\n",
       "  -0.03403707966208458,\n",
       "  -0.4788098931312561,\n",
       "  0.026493653655052185,\n",
       "  0.17659075558185577,\n",
       "  0.1522066295146942,\n",
       "  0.12649881839752197,\n",
       "  -0.09748749434947968,\n",
       "  -0.05796892195940018,\n",
       "  -0.16107767820358276,\n",
       "  -0.03448998183012009,\n",
       "  -0.3419411778450012,\n",
       "  0.019385896623134613,\n",
       "  0.4549376666545868,\n",
       "  0.19021837413311005,\n",
       "  0.36919671297073364,\n",
       "  -0.3552793562412262,\n",
       "  -0.05956146866083145,\n",
       "  -0.16934947669506073,\n",
       "  -0.37183624505996704,\n",
       "  0.34409278631210327,\n",
       "  0.3735753893852234,\n",
       "  0.6145904660224915,\n",
       "  0.36276566982269287,\n",
       "  0.07754044234752655,\n",
       "  -0.10203584283590317,\n",
       "  0.3664337992668152,\n",
       "  0.33005452156066895,\n",
       "  0.23062773048877716,\n",
       "  -0.08381249010562897,\n",
       "  -0.6152686476707458,\n",
       "  0.08016038686037064,\n",
       "  0.4226840138435364,\n",
       "  0.20513348281383514,\n",
       "  0.20969519019126892,\n",
       "  0.006733208894729614,\n",
       "  0.3966979384422302,\n",
       "  0.06396112591028214,\n",
       "  0.01322111114859581,\n",
       "  0.47956421971321106,\n",
       "  0.09746244549751282,\n",
       "  0.554295539855957,\n",
       "  -0.17209020256996155,\n",
       "  -0.3831581175327301,\n",
       "  0.16941651701927185,\n",
       "  -0.05834444984793663,\n",
       "  -0.4284006655216217,\n",
       "  0.20822149515151978,\n",
       "  -0.04211413860321045,\n",
       "  0.5877148509025574,\n",
       "  0.1640811711549759,\n",
       "  0.4046086370944977,\n",
       "  -0.47605443000793457,\n",
       "  0.22976435720920563,\n",
       "  0.4764801561832428,\n",
       "  0.04466218128800392,\n",
       "  -0.5509579181671143,\n",
       "  -0.16142867505550385,\n",
       "  0.050668854266405106,\n",
       "  -0.11284532397985458,\n",
       "  1.4089257717132568,\n",
       "  0.2194490134716034,\n",
       "  0.12663564085960388,\n",
       "  -0.23390698432922363,\n",
       "  -0.213749498128891,\n",
       "  -0.231299489736557,\n",
       "  -0.04921070113778114,\n",
       "  0.05132631957530975,\n",
       "  -0.4341212511062622,\n",
       "  0.09573392570018768,\n",
       "  -0.0041267890483140945,\n",
       "  0.17218363285064697,\n",
       "  0.22440075874328613,\n",
       "  -0.27191412448883057,\n",
       "  -0.14799776673316956,\n",
       "  0.3181843161582947,\n",
       "  0.05727441608905792,\n",
       "  -0.15463918447494507,\n",
       "  0.24315914511680603,\n",
       "  -0.07736445218324661,\n",
       "  -0.5358003377914429,\n",
       "  -0.3752957880496979,\n",
       "  0.7265118360519409,\n",
       "  -0.23083579540252686,\n",
       "  -2.2895030975341797,\n",
       "  -0.05466612055897713,\n",
       "  -0.07213261723518372,\n",
       "  -0.4354923665523529,\n",
       "  0.4714668393135071,\n",
       "  0.03714495897293091,\n",
       "  0.02927839569747448,\n",
       "  -0.2406580150127411,\n",
       "  -0.171986386179924,\n",
       "  0.3191584646701813,\n",
       "  -0.13990649580955505,\n",
       "  0.25767624378204346,\n",
       "  0.5971342325210571,\n",
       "  -0.3020985424518585,\n",
       "  0.2426801174879074,\n",
       "  -0.39346909523010254,\n",
       "  0.5476452112197876,\n",
       "  0.25139036774635315,\n",
       "  -0.34946274757385254,\n",
       "  0.37694689631462097,\n",
       "  -0.06286925822496414,\n",
       "  0.3373461663722992,\n",
       "  -0.21579205989837646,\n",
       "  0.17566542327404022,\n",
       "  -0.10209043323993683,\n",
       "  0.10545843839645386,\n",
       "  -0.12319131195545197,\n",
       "  -0.03941557928919792,\n",
       "  0.1338617205619812,\n",
       "  -0.33338260650634766,\n",
       "  -0.24634192883968353,\n",
       "  -0.06460820883512497,\n",
       "  -0.0693427100777626,\n",
       "  0.10302869975566864,\n",
       "  0.13014934957027435,\n",
       "  -0.6515365839004517,\n",
       "  -0.7087113857269287,\n",
       "  0.0621064156293869,\n",
       "  -0.19440627098083496,\n",
       "  -0.26389971375465393,\n",
       "  -0.1815519630908966,\n",
       "  0.07101349532604218,\n",
       "  -0.24858202040195465,\n",
       "  -0.0189933143556118,\n",
       "  -4.27621603012085,\n",
       "  0.3491930067539215,\n",
       "  -0.034844860434532166,\n",
       "  0.10935994982719421,\n",
       "  -0.0038950545713305473,\n",
       "  -0.18403160572052002,\n",
       "  0.2975444495677948,\n",
       "  0.021314766258001328,\n",
       "  0.43775320053100586,\n",
       "  -0.5134261250495911,\n",
       "  0.5497733354568481,\n",
       "  0.08497751504182816,\n",
       "  -0.11308924108743668,\n",
       "  0.36141303181648254,\n",
       "  -0.34362107515335083,\n",
       "  0.37910568714141846,\n",
       "  -0.004803533665835857,\n",
       "  -0.08110585808753967,\n",
       "  -0.7406610250473022,\n",
       "  0.04904146492481232,\n",
       "  0.4916157126426697,\n",
       "  0.1438521295785904,\n",
       "  -0.08477354049682617,\n",
       "  0.22793173789978027,\n",
       "  0.09651179611682892,\n",
       "  -0.3868396282196045,\n",
       "  -0.24563489854335785,\n",
       "  -0.40833979845046997,\n",
       "  0.32079192996025085,\n",
       "  0.1074095368385315,\n",
       "  0.20362034440040588,\n",
       "  -0.2199530452489853,\n",
       "  -0.005968758836388588,\n",
       "  -0.038321636617183685,\n",
       "  -0.10105179250240326,\n",
       "  -2.6045265197753906,\n",
       "  -0.11248897016048431,\n",
       "  -0.4355575442314148,\n",
       "  -0.14244724810123444,\n",
       "  0.11522568762302399,\n",
       "  -0.040064629167318344,\n",
       "  0.42676103115081787,\n",
       "  0.11063580214977264,\n",
       "  -0.5422403216362,\n",
       "  0.19992122054100037,\n",
       "  0.47917115688323975,\n",
       "  -0.10642768442630768,\n",
       "  -0.13243648409843445,\n",
       "  0.33416393399238586,\n",
       "  0.1658397912979126,\n",
       "  0.060369331389665604,\n",
       "  0.539715588092804,\n",
       "  -0.06551329791545868,\n",
       "  0.15776951611042023,\n",
       "  0.295393168926239,\n",
       "  -0.2622193694114685,\n",
       "  0.3306376338005066,\n",
       "  -0.2181399017572403,\n",
       "  -0.1941208392381668,\n",
       "  0.4509548842906952,\n",
       "  0.15519818663597107,\n",
       "  -0.4027468264102936,\n",
       "  -0.24456579983234406,\n",
       "  -0.0996730625629425,\n",
       "  0.5591458678245544,\n",
       "  -0.10760006308555603,\n",
       "  -0.027084872126579285,\n",
       "  0.10111738741397858,\n",
       "  -0.22805553674697876,\n",
       "  -0.2633117735385895,\n",
       "  -0.23819664120674133,\n",
       "  0.36750537157058716,\n",
       "  0.27362000942230225,\n",
       "  0.5604189038276672,\n",
       "  -0.06099579110741615,\n",
       "  -0.2937348186969757,\n",
       "  0.6034168004989624,\n",
       "  0.32285505533218384,\n",
       "  0.23122000694274902,\n",
       "  0.4951120913028717,\n",
       "  -0.10171161592006683,\n",
       "  0.4697363078594208,\n",
       "  0.07785242050886154,\n",
       "  0.22443795204162598,\n",
       "  -0.07628375291824341,\n",
       "  -0.09363457560539246,\n",
       "  -0.07617802172899246,\n",
       "  1.1836401224136353,\n",
       "  0.16180306673049927,\n",
       "  -0.0020402371883392334,\n",
       "  -0.3612707853317261,\n",
       "  0.15069055557250977,\n",
       "  0.41776469349861145,\n",
       "  0.08073809742927551,\n",
       "  0.17877018451690674,\n",
       "  0.3583444058895111,\n",
       "  -0.3887202739715576,\n",
       "  0.32815882563591003,\n",
       "  -0.0458199605345726,\n",
       "  -0.04479549080133438,\n",
       "  -0.25629356503486633,\n",
       "  0.07147824764251709,\n",
       "  -0.7862069606781006,\n",
       "  -0.06795912235975266,\n",
       "  0.0317281074821949,\n",
       "  0.23672381043434143,\n",
       "  0.2134360373020172,\n",
       "  -0.38680046796798706,\n",
       "  -1.1987909078598022,\n",
       "  0.1008550226688385,\n",
       "  0.10443992912769318,\n",
       "  -0.5324422121047974,\n",
       "  -0.01468949019908905,\n",
       "  0.16359150409698486,\n",
       "  -0.11690736562013626,\n",
       "  0.16090014576911926,\n",
       "  0.14086677134037018,\n",
       "  -0.48384952545166016,\n",
       "  0.4799211621284485,\n",
       "  -0.24165460467338562,\n",
       "  -0.23070064187049866,\n",
       "  0.19233745336532593,\n",
       "  -0.30793139338493347,\n",
       "  -0.34900230169296265,\n",
       "  -0.03655620664358139,\n",
       "  -0.07058397680521011,\n",
       "  0.09847458451986313,\n",
       "  0.3196744918823242,\n",
       "  0.22167636454105377,\n",
       "  -0.2468700259923935,\n",
       "  -0.09542974829673767,\n",
       "  0.038167960941791534,\n",
       "  -0.21825972199440002,\n",
       "  0.2193998545408249,\n",
       "  -0.4009544849395752,\n",
       "  -0.3769785761833191,\n",
       "  -0.41270336508750916,\n",
       "  -0.16846108436584473,\n",
       "  0.2202502340078354,\n",
       "  -0.11885572969913483,\n",
       "  -0.23411332070827484,\n",
       "  0.06591606885194778,\n",
       "  0.08428084850311279,\n",
       "  0.2999187111854553,\n",
       "  0.22404798865318298,\n",
       "  -0.498248815536499,\n",
       "  -0.43303877115249634,\n",
       "  0.47135528922080994,\n",
       "  0.32968437671661377,\n",
       "  0.46684780716896057,\n",
       "  -0.17155125737190247,\n",
       "  -0.18745726346969604,\n",
       "  0.29113349318504333,\n",
       "  0.12572696805000305,\n",
       "  0.20368891954421997,\n",
       "  0.3368757665157318,\n",
       "  0.29943448305130005,\n",
       "  -0.3280720114707947,\n",
       "  -0.6824895739555359,\n",
       "  -0.8473179936408997,\n",
       "  0.09202747046947479,\n",
       "  0.19384464621543884,\n",
       "  -0.030717380344867706,\n",
       "  -0.6453896760940552,\n",
       "  -0.23727037012577057,\n",
       "  0.03191336616873741,\n",
       "  0.14015671610832214,\n",
       "  -0.40677574276924133,\n",
       "  -0.5037364959716797,\n",
       "  -0.03910040110349655,\n",
       "  -0.44376665353775024,\n",
       "  -0.16566674411296844,\n",
       "  -0.0768161341547966,\n",
       "  0.22067123651504517,\n",
       "  0.06574834883213043,\n",
       "  0.18114590644836426,\n",
       "  -0.1817292422056198,\n",
       "  -0.30580297112464905,\n",
       "  0.18406079709529877,\n",
       "  -0.2754296362400055,\n",
       "  0.41177481412887573,\n",
       "  0.20637217164039612,\n",
       "  0.38867977261543274,\n",
       "  -0.5244316458702087,\n",
       "  0.64081209897995,\n",
       "  0.20380645990371704,\n",
       "  -0.24995474517345428,\n",
       "  -0.1635475903749466,\n",
       "  -0.41904744505882263,\n",
       "  0.2892701029777527,\n",
       "  0.22473353147506714,\n",
       "  0.27396804094314575,\n",
       "  -0.005413185805082321,\n",
       "  0.15212112665176392,\n",
       "  -0.10128582268953323,\n",
       "  0.09591680765151978,\n",
       "  -0.06884898990392685,\n",
       "  -1.219611644744873,\n",
       "  0.1832139790058136,\n",
       "  0.8639726042747498,\n",
       "  0.38958975672721863,\n",
       "  0.23705586791038513,\n",
       "  -0.11103855073451996,\n",
       "  -0.8064706325531006,\n",
       "  0.768859326839447,\n",
       "  0.28218570351600647,\n",
       "  -0.05049770325422287,\n",
       "  -0.39266014099121094,\n",
       "  -0.05262887477874756,\n",
       "  0.37329381704330444,\n",
       "  0.06751847267150879,\n",
       "  0.18180039525032043,\n",
       "  0.18848048150539398,\n",
       "  -0.20089569687843323,\n",
       "  -0.5001535415649414,\n",
       "  -5.661160685122013e-05,\n",
       "  -0.14203177392482758,\n",
       "  0.12176273763179779,\n",
       "  0.007294571027159691,\n",
       "  -0.0937909334897995,\n",
       "  -0.09061833471059799,\n",
       "  -0.243047833442688,\n",
       "  0.25922277569770813,\n",
       "  0.12743334472179413,\n",
       "  0.4623608887195587,\n",
       "  0.3905152678489685,\n",
       "  0.32108545303344727,\n",
       "  -0.314868688583374,\n",
       "  -0.31697365641593933,\n",
       "  -0.4973321557044983,\n",
       "  -0.08806861191987991,\n",
       "  0.10161729156970978,\n",
       "  -0.12825196981430054,\n",
       "  -0.08937909454107285,\n",
       "  -0.4463037848472595,\n",
       "  0.3553372025489807,\n",
       "  0.4967261850833893,\n",
       "  -0.3190765678882599,\n",
       "  0.5232613682746887,\n",
       "  0.600888729095459,\n",
       "  0.07063674926757812,\n",
       "  0.4644312262535095,\n",
       "  0.5688386559486389,\n",
       "  -0.538316547870636,\n",
       "  0.34072068333625793,\n",
       "  0.2327345907688141,\n",
       "  -0.5999836325645447,\n",
       "  -0.04464252293109894,\n",
       "  -0.4628836214542389,\n",
       "  -0.38775962591171265,\n",
       "  0.11192357540130615,\n",
       "  0.1864231526851654,\n",
       "  -0.09039313346147537,\n",
       "  0.24008861184120178,\n",
       "  -0.2743973135948181,\n",
       "  -0.3771529197692871,\n",
       "  -0.14571551978588104,\n",
       "  0.31992441415786743,\n",
       "  -0.39819177985191345,\n",
       "  -0.6604433655738831,\n",
       "  -0.206429123878479,\n",
       "  -0.24493719637393951,\n",
       "  -0.4066571295261383,\n",
       "  -0.2685782015323639,\n",
       "  -0.2346973419189453,\n",
       "  -0.3734778165817261,\n",
       "  0.2771880030632019,\n",
       "  0.6246726512908936,\n",
       "  -0.160180926322937,\n",
       "  -0.3972540497779846,\n",
       "  0.3833913803100586,\n",
       "  -0.40293192863464355,\n",
       "  0.1679404079914093,\n",
       "  0.5374852418899536,\n",
       "  0.01932176947593689,\n",
       "  0.1636580228805542,\n",
       "  -0.28463152050971985,\n",
       "  -0.4403904676437378,\n",
       "  -0.14410319924354553,\n",
       "  -0.18190205097198486,\n",
       "  0.16071146726608276,\n",
       "  0.5070899128913879,\n",
       "  0.41603586077690125,\n",
       "  -0.2575250267982483,\n",
       "  -0.3115245997905731,\n",
       "  0.1586470603942871,\n",
       "  -0.6833130121231079,\n",
       "  -0.3394542634487152,\n",
       "  -0.13704928755760193,\n",
       "  0.5498906373977661,\n",
       "  -0.15394410490989685,\n",
       "  0.1363644152879715,\n",
       "  0.1296594887971878,\n",
       "  0.6406440138816833,\n",
       "  -0.29348522424697876,\n",
       "  0.291787326335907,\n",
       "  0.07594078779220581,\n",
       "  -0.5135697722434998,\n",
       "  0.5349115133285522,\n",
       "  -0.020365655422210693,\n",
       "  0.3995261490345001,\n",
       "  -0.0834403783082962,\n",
       "  -0.0387335941195488,\n",
       "  0.5760572552680969,\n",
       "  0.01684676855802536,\n",
       "  -0.1745951622724533,\n",
       "  -0.19818681478500366,\n",
       "  -0.06705912947654724,\n",
       "  -0.005252506583929062,\n",
       "  -0.4102088212966919,\n",
       "  -0.46289581060409546,\n",
       "  0.18076655268669128,\n",
       "  -0.32645368576049805,\n",
       "  0.12504595518112183,\n",
       "  -0.4723469614982605,\n",
       "  2.478156089782715,\n",
       "  0.5122138261795044,\n",
       "  0.09800463169813156,\n",
       "  -0.3392018675804138,\n",
       "  0.14022406935691833,\n",
       "  0.023879965767264366,\n",
       "  -0.1313989758491516,\n",
       "  0.3305228054523468,\n",
       "  -0.1596546620130539,\n",
       "  0.2555879056453705,\n",
       "  -0.31216636300086975,\n",
       "  -0.06149958819150925,\n",
       "  -0.027204884216189384,\n",
       "  0.33929112553596497,\n",
       "  0.279887855052948,\n",
       "  -0.020406058058142662,\n",
       "  -0.11051179468631744,\n",
       "  -0.28348609805107117,\n",
       "  -0.9630566239356995,\n",
       "  -0.028546538203954697,\n",
       "  -0.5808510780334473,\n",
       "  0.28403767943382263,\n",
       "  0.3453340232372284,\n",
       "  -0.3891565799713135,\n",
       "  0.293944388628006,\n",
       "  0.4687522053718567,\n",
       "  0.2349177896976471,\n",
       "  0.3133637309074402,\n",
       "  0.16924959421157837,\n",
       "  0.22561141848564148,\n",
       "  -0.2685922682285309,\n",
       "  -0.026972882449626923,\n",
       "  0.1963711529970169,\n",
       "  0.4332641661167145,\n",
       "  -0.4216077923774719,\n",
       "  -0.13412372767925262,\n",
       "  0.008579438552260399,\n",
       "  -0.46905624866485596,\n",
       "  0.12107041478157043,\n",
       "  0.17350704967975616,\n",
       "  0.09784162044525146,\n",
       "  -0.1536332368850708,\n",
       "  0.26271364092826843,\n",
       "  0.0864400863647461,\n",
       "  -0.21719138324260712,\n",
       "  0.42508664727211,\n",
       "  -0.23016516864299774,\n",
       "  -0.27453070878982544,\n",
       "  0.6457961797714233,\n",
       "  0.33540448546409607,\n",
       "  0.13354121148586273,\n",
       "  -0.2965890169143677,\n",
       "  -0.3993431627750397,\n",
       "  0.31787583231925964,\n",
       "  -0.2700046896934509,\n",
       "  -0.1278126835823059,\n",
       "  0.4569051265716553,\n",
       "  0.10835257172584534,\n",
       "  -0.26076540350914,\n",
       "  0.5123145580291748,\n",
       "  -0.1080753281712532,\n",
       "  0.43491533398628235,\n",
       "  0.38915249705314636,\n",
       "  -0.13751308619976044,\n",
       "  -0.2606460750102997,\n",
       "  0.14887072145938873,\n",
       "  -0.2624771296977997,\n",
       "  0.06823937594890594,\n",
       "  0.08158646523952484,\n",
       "  0.034169115126132965,\n",
       "  -0.061244744807481766,\n",
       "  0.22399073839187622,\n",
       "  0.44674476981163025,\n",
       "  0.2801358103752136,\n",
       "  0.24298202991485596,\n",
       "  -0.25213339924812317,\n",
       "  0.4494190514087677,\n",
       "  -0.1577770859003067,\n",
       "  -0.32540029287338257,\n",
       "  -3.228605031967163,\n",
       "  0.13088378310203552,\n",
       "  0.15063899755477905,\n",
       "  0.2751023471355438,\n",
       "  0.08115313947200775,\n",
       "  0.289010614156723,\n",
       "  0.6362525820732117,\n",
       "  -0.029711494222283363,\n",
       "  0.18516002595424652,\n",
       "  -0.13056661188602448,\n",
       "  0.36888766288757324,\n",
       "  -0.01620003767311573,\n",
       "  0.333093523979187,\n",
       "  -0.055287472903728485,\n",
       "  0.43725332617759705,\n",
       "  0.2257026731967926,\n",
       "  0.5722171664237976,\n",
       "  -0.4665887951850891,\n",
       "  0.3907971978187561,\n",
       "  -0.2153826802968979,\n",
       "  -0.05805375427007675,\n",
       "  -9.6932053565979e-05,\n",
       "  0.12163487821817398,\n",
       "  -0.4223216772079468,\n",
       "  -0.8033581972122192,\n",
       "  0.6466439962387085,\n",
       "  0.01683235913515091,\n",
       "  -0.24694350361824036,\n",
       "  0.12824498116970062,\n",
       "  0.4348289370536804,\n",
       "  -0.00534205324947834,\n",
       "  0.7144278287887573,\n",
       "  0.04141271114349365,\n",
       "  0.06242058426141739,\n",
       "  0.004860091954469681,\n",
       "  -0.017550010234117508,\n",
       "  -0.5638316869735718,\n",
       "  0.003081381320953369,\n",
       "  0.6520841121673584,\n",
       "  0.0898275300860405,\n",
       "  -0.14441336691379547,\n",
       "  0.49317601323127747,\n",
       "  0.1038130670785904,\n",
       "  0.08881238102912903,\n",
       "  0.10248418152332306,\n",
       "  -0.25115683674812317,\n",
       "  0.6370594501495361,\n",
       "  -0.09109501540660858,\n",
       "  0.39780575037002563,\n",
       "  -0.12641456723213196,\n",
       "  0.13300062716007233,\n",
       "  -0.020047515630722046,\n",
       "  0.06397232413291931,\n",
       "  0.04295256733894348,\n",
       "  0.22989101707935333,\n",
       "  -0.07248511910438538,\n",
       "  0.33080315589904785,\n",
       "  0.14776508510112762,\n",
       "  -0.009311376139521599,\n",
       "  -0.4689696729183197,\n",
       "  -0.031027883291244507,\n",
       "  0.14482006430625916,\n",
       "  -0.20320145785808563,\n",
       "  -0.15278705954551697,\n",
       "  0.3958542048931122,\n",
       "  -0.11006064713001251,\n",
       "  0.5044205784797668,\n",
       "  0.02023226022720337,\n",
       "  -0.24724522233009338,\n",
       "  -0.116994708776474,\n",
       "  -0.3287373185157776,\n",
       "  0.01735679991543293,\n",
       "  0.45477765798568726,\n",
       "  -0.2742362916469574,\n",
       "  -0.17394039034843445,\n",
       "  0.13284815847873688,\n",
       "  0.41555890440940857,\n",
       "  0.31142860651016235,\n",
       "  0.2827698886394501,\n",
       "  0.0911903828382492,\n",
       "  -0.24268241226673126,\n",
       "  0.06625376641750336,\n",
       "  -0.23155105113983154,\n",
       "  0.0782463550567627,\n",
       "  -0.0002243630588054657,\n",
       "  -8.358695983886719,\n",
       "  -0.21133992075920105,\n",
       "  -0.25348547101020813,\n",
       "  -0.31473395228385925,\n",
       "  -0.13726581633090973,\n",
       "  -0.17229247093200684,\n",
       "  -0.020719371736049652,\n",
       "  -0.4215872585773468,\n",
       "  0.19091638922691345,\n",
       "  -0.29702505469322205,\n",
       "  -0.059996772557497025,\n",
       "  0.0006593950092792511,\n",
       "  0.008842162787914276,\n",
       "  -0.3205048143863678,\n",
       "  0.29852941632270813,\n",
       "  0.6004313826560974],\n",
       " 'loss': 0.7738249897956848}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.predict(\"This thing is pretty cool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"toxic-train-clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "l = len(df)\n",
    "def get_vector(text):\n",
    "    global i\n",
    "    global l\n",
    "    i+=1\n",
    "    if i % 1000 == 0:\n",
    "        print(i, \"/\", l)\n",
    "    return tagger.predict(text)[\"state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 159571\n",
      "2000 / 159571\n",
      "3000 / 159571\n",
      "4000 / 159571\n",
      "5000 / 159571\n",
      "6000 / 159571\n",
      "7000 / 159571\n",
      "8000 / 159571\n",
      "9000 / 159571\n",
      "10000 / 159571\n",
      "11000 / 159571\n",
      "12000 / 159571\n",
      "13000 / 159571\n",
      "14000 / 159571\n",
      "15000 / 159571\n",
      "16000 / 159571\n",
      "17000 / 159571\n",
      "18000 / 159571\n",
      "19000 / 159571\n",
      "20000 / 159571\n",
      "21000 / 159571\n",
      "22000 / 159571\n",
      "23000 / 159571\n",
      "24000 / 159571\n",
      "25000 / 159571\n",
      "26000 / 159571\n",
      "27000 / 159571\n",
      "28000 / 159571\n",
      "29000 / 159571\n",
      "30000 / 159571\n",
      "31000 / 159571\n",
      "32000 / 159571\n",
      "33000 / 159571\n",
      "34000 / 159571\n",
      "35000 / 159571\n",
      "36000 / 159571\n",
      "37000 / 159571\n",
      "38000 / 159571\n",
      "39000 / 159571\n",
      "40000 / 159571\n",
      "41000 / 159571\n",
      "42000 / 159571\n",
      "43000 / 159571\n",
      "44000 / 159571\n",
      "45000 / 159571\n",
      "46000 / 159571\n",
      "47000 / 159571\n",
      "48000 / 159571\n",
      "49000 / 159571\n",
      "50000 / 159571\n",
      "51000 / 159571\n",
      "52000 / 159571\n",
      "53000 / 159571\n",
      "54000 / 159571\n",
      "55000 / 159571\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-c32c55e261a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"comment_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3591\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3593\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-340486b3cb88>\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"state\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\allennlp\\predictors\\sentence_tagger.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"sentence\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\allennlp\\predictors\\predictor.py\u001b[0m in \u001b[0;36mpredict_json\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0minstance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_json_to_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjson_to_labeled_instances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInstance\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\allennlp\\predictors\\predictor.py\u001b[0m in \u001b[0;36mpredict_instance\u001b[1;34m(self, instance)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mInstance\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_on_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msanitize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\allennlp\\models\\model.py\u001b[0m in \u001b[0;36mforward_on_instance\u001b[1;34m(self, instance)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensors\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minto\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mremove\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0mdimension\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \"\"\"\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_on_instances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     def forward_on_instances(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\allennlp\\models\\model.py\u001b[0m in \u001b[0;36mforward_on_instances\u001b[1;34m(self, instances)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_instances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0mmodel_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0minstance_separated_output\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstances\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-abe2f1119695>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tokens, id, label)\u001b[0m\n\u001b[0;32m     12\u001b[0m                 id, label):\n\u001b[0;32m     13\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_text_field_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mclass_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\allennlp\\modules\\text_field_embedders\\basic_text_field_embedder.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, text_field_input, num_wrapping_dims, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m                 \u001b[1;31m# is bijective and just use the key directly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext_field_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m                 \u001b[0mtoken_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mforward_params_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m             \u001b[0membedded_representations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_representations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\allennlp\\modules\\token_embedders\\bert_token_embedder.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, offsets, token_type_ids)\u001b[0m\n\u001b[0;32m    173\u001b[0m         all_encoder_layers, _ = self.bert_model(input_ids=util.combine_initial_dims(input_ids),\n\u001b[0;32m    174\u001b[0m                                                 \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombine_initial_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                                                 attention_mask=util.combine_initial_dims(input_mask))\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_encoder_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\pytorch_pretrained_bert\\modeling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[0;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[0;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 733\u001b[1;33m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[0;32m    734\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\pytorch_pretrained_bert\\modeling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\pytorch_pretrained_bert\\modeling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\pytorch_pretrained_bert\\modeling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1372\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1373\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings = np.stack(df[\"comment_text\"].apply(get_vector).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('toxic_bert_matrix.out', embeddings, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
