{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from overrides import overrides\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.fields import TextField, MetadataField, ArrayField\n",
    "from allennlp.nn.util import get_text_field_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "        \n",
    "config = Config(\n",
    "    testing=True,\n",
    "    seed=1,\n",
    "    batch_size=64,\n",
    "    lr=3e-4,\n",
    "    epochs=2,\n",
    "    hidden_sz=64,\n",
    "    max_seq_len=100, # necessary to limit memory usage\n",
    "    max_vocab_size=100000,\n",
    ")\n",
    "\n",
    "class JigsawDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer=lambda x: x.split(),\n",
    "                 token_indexers=None,\n",
    "                 max_seq_len=config.max_seq_len):\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    " \n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens, id=None, labels=None):\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "         \n",
    "        id_field = MetadataField(id)\n",
    "        fields[\"id\"] = id_field\n",
    "         \n",
    "        if labels is None:\n",
    "            labels = np.zeros(len(label_cols))\n",
    "        label_field = ArrayField(array=labels)\n",
    "        fields[\"label\"] = label_field\n",
    " \n",
    "        return Instance(fields)\n",
    "     \n",
    "    @overrides\n",
    "    def _read(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if config.testing: df = df.head(1000)\n",
    "        for i, row in df.iterrows():\n",
    "            yield self.text_to_instance(\n",
    "                [Token(x) for x in self.tokenizer(row[\"comment_text\"])],\n",
    "                row[\"id\"], row[label_cols].values,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:10, 97.78it/s]\n"
     ]
    }
   ],
   "source": [
    "token_indexer = SingleIdTokenIndexer()\n",
    "\n",
    "def tokenizer(x: str):\n",
    "    return [w.text for w in\n",
    "            SpacyWordSplitter(language='en_core_web_lg', \n",
    "                              pos_tags=False).split_words(x)[:config.max_seq_len]]\n",
    "\n",
    "# the token indexer is responsible for mapping tokens to integers\n",
    "token_indexer = SingleIdTokenIndexer()\n",
    "\n",
    "reader = JigsawDatasetReader(\n",
    "    tokenizer=tokenizer,\n",
    "    token_indexers={\"tokens\": token_indexer}\n",
    ")\n",
    "\n",
    "# Note: this csv contains the same information as toxic-train, but the comment text has been cleaned.\n",
    "train = reader.read(\"toxic-train-clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 24457.01it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train, max_vocab_size=config.max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(Model):\n",
    "    def __init__(self, word_embeddings,\n",
    "                 encoder,\n",
    "                 out_sz=len(label_cols)):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.projection = nn.Linear(self.encoder.get_output_dim(), out_sz)\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "         \n",
    "    def forward(self, tokens,\n",
    "                id, label):\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        state = self.encoder(embeddings, mask)\n",
    "        class_logits = self.projection(state)\n",
    "         \n",
    "        output = {\"class_logits\": class_logits, \"state\": state}\n",
    "        output[\"loss\"] = self.loss(class_logits, label)\n",
    " \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = Embedding(num_embeddings=config.max_vocab_size + 2,\n",
    "                            embedding_dim=300, padding_index=0)\n",
    "\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PytorchSeq2VecWrapper(nn.LSTM(300, config.hidden_sz, bidirectional=True, batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: model can be saved via\n",
    "#    torch.save(model.state_dict(), \"toxic_RNN.pt\")\n",
    "\n",
    "model = BaselineModel(word_embeddings, encoder)\n",
    "model.load_state_dict(torch.load('toxic_RNN.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.sentence_tagger import SentenceTaggerPredictor\n",
    "tagger = SentenceTaggerPredictor(model, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_logits': [-5.918819427490234,\n",
       "  -8.09105396270752,\n",
       "  -8.28278636932373,\n",
       "  -7.772166728973389,\n",
       "  -8.256393432617188,\n",
       "  -8.967432022094727],\n",
       " 'state': [0.9399575591087341,\n",
       "  -0.9350807070732117,\n",
       "  0.956086277961731,\n",
       "  -0.9590500593185425,\n",
       "  -0.13831201195716858,\n",
       "  -0.7496212124824524,\n",
       "  0.9502302408218384,\n",
       "  -0.8554535508155823,\n",
       "  0.9269254803657532,\n",
       "  0.9353356957435608,\n",
       "  0.9392806887626648,\n",
       "  0.817321240901947,\n",
       "  0.9371330142021179,\n",
       "  -0.9617658853530884,\n",
       "  0.9054552316665649,\n",
       "  -0.9566709995269775,\n",
       "  0.9256322979927063,\n",
       "  -0.9423669576644897,\n",
       "  0.7850815653800964,\n",
       "  0.8274006247520447,\n",
       "  -0.9281306266784668,\n",
       "  -0.931768536567688,\n",
       "  -0.9299911260604858,\n",
       "  -0.8938233256340027,\n",
       "  -0.95037442445755,\n",
       "  -0.8536920547485352,\n",
       "  -0.9124063849449158,\n",
       "  -0.8764339089393616,\n",
       "  0.8114773035049438,\n",
       "  -0.8715393543243408,\n",
       "  0.9086158275604248,\n",
       "  -0.9413347840309143,\n",
       "  0.9197977185249329,\n",
       "  0.8901531100273132,\n",
       "  0.8719948530197144,\n",
       "  -0.9166281223297119,\n",
       "  0.871108889579773,\n",
       "  -0.9222814440727234,\n",
       "  -0.8468930125236511,\n",
       "  0.9559369087219238,\n",
       "  -0.9426683783531189,\n",
       "  -0.8540045022964478,\n",
       "  -0.9315512180328369,\n",
       "  -0.938119113445282,\n",
       "  -0.9355818629264832,\n",
       "  0.9027148485183716,\n",
       "  -0.9572973251342773,\n",
       "  -0.9690073132514954,\n",
       "  0.9024527668952942,\n",
       "  0.9370691776275635,\n",
       "  0.9571402668952942,\n",
       "  0.9808005094528198,\n",
       "  -0.9577116966247559,\n",
       "  -0.970082700252533,\n",
       "  0.8091335296630859,\n",
       "  0.9707780480384827,\n",
       "  0.9683182239532471,\n",
       "  0.6077048778533936,\n",
       "  0.853166401386261,\n",
       "  -0.8022432923316956,\n",
       "  0.9507216215133667,\n",
       "  0.8483584523200989,\n",
       "  -0.8894922733306885,\n",
       "  -0.9766219854354858,\n",
       "  0.9771242141723633,\n",
       "  0.9737865328788757,\n",
       "  0.9549621343612671,\n",
       "  -0.9489642977714539,\n",
       "  -0.9508726000785828,\n",
       "  0.9456693530082703,\n",
       "  -0.9483768343925476,\n",
       "  -0.9805428385734558,\n",
       "  0.9738043546676636,\n",
       "  0.9865292906761169,\n",
       "  0.9329075217247009,\n",
       "  -0.9485565423965454,\n",
       "  0.9588866829872131,\n",
       "  -0.9806221127510071,\n",
       "  0.9594225287437439,\n",
       "  0.9722524285316467,\n",
       "  -0.9583702683448792,\n",
       "  -0.9793074727058411,\n",
       "  -0.9087265729904175,\n",
       "  -0.9625321626663208,\n",
       "  -0.8317729830741882,\n",
       "  -0.9302815794944763,\n",
       "  -0.9457480311393738,\n",
       "  -0.9460344314575195,\n",
       "  -0.9758962392807007,\n",
       "  0.9607629776000977,\n",
       "  -0.9621794819831848,\n",
       "  0.952586829662323,\n",
       "  -0.8107766509056091,\n",
       "  -0.8960327506065369,\n",
       "  -0.4089009165763855,\n",
       "  0.9382245540618896,\n",
       "  -0.9320488572120667,\n",
       "  -0.977425754070282,\n",
       "  -0.8961072564125061,\n",
       "  -0.991357147693634,\n",
       "  -0.8791350722312927,\n",
       "  0.9871906638145447,\n",
       "  0.9511215090751648,\n",
       "  0.9607012867927551,\n",
       "  0.906525194644928,\n",
       "  0.9715924263000488,\n",
       "  0.9198601841926575,\n",
       "  0.898988664150238,\n",
       "  -0.9488182663917542,\n",
       "  0.9578496217727661,\n",
       "  0.9516460299491882,\n",
       "  0.9146450161933899,\n",
       "  0.9780887365341187,\n",
       "  0.9134013652801514,\n",
       "  0.9797056913375854,\n",
       "  -0.9698135256767273,\n",
       "  -0.9702672958374023,\n",
       "  0.956787645816803,\n",
       "  0.9447128772735596,\n",
       "  -0.9265062212944031,\n",
       "  0.9486256837844849,\n",
       "  -0.9731996655464172,\n",
       "  0.9586142301559448,\n",
       "  0.9436403512954712,\n",
       "  0.9649695754051208,\n",
       "  -0.9419906735420227,\n",
       "  -0.9796139001846313,\n",
       "  0.9679533839225769],\n",
       " 'loss': 0.000675360148306936}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: class_logits correspond to predictions,\n",
    "# state corresponds to the encoding.\n",
    "tagger.predict(\"This thing is pretty cool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"toxic-train-clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(text):\n",
    "    try:\n",
    "        return tagger.predict(text)[\"state\"]\n",
    "    except Exception:\n",
    "        return [0] * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.stack(df[\"comment_text\"].apply(get_vector).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('toxic_rnn_matrix.out', embeddings, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[:, 2:8]\n",
    "x_matrix = np.loadtxt('toxic_rnn_matrix.out', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_matrix, y, test_size= 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
