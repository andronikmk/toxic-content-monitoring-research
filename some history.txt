The initial version of the project had three essential components. The first was the API, which was finished more than adequately. The second was a dataset incorporating suicidality predictions. The third was the predictive model. This document traces out some of the histories of these last two components.

For the dataset, we tried two main approaches. The first was to take a preexisting dataset, called the "Dataset of depressive and suicidal posts" (https://data.mendeley.com/datasets/838dbcjpxb/1) into a training set. This was done and is available in this repository. The original dataset was in Russian, so this was translated. After being incorporated into the dataset, it was suspiciously easy to pick out the posts, even if the most basic TFIDF encoding was used, and even if other, innocuous, translated Russian was mixed in. We never figured out the problem, but it's enough evidence to suggest something fundamentally wrong with the approach. Alternatively, posts were also scrapped from the r/SuicideWatch subreddit. Those posts didn't have the same classification problem. Ultimately, we weren't able to confidently get a complete dataset, but some progress was made, and other potential directions are listed in the Notion doc.

For the model, we started out by taking a TFIDF vectorizer, and Elmo vectorizer, and a BERT vectorizer, and tried to use a wide variety of different classifiers (logistic regression, random forests, balanced bagging, etc.) on each. Most performed very poorly, with almost none being able to outperform a logistic regression on a TFIDF vectorization. Ultimately, we used a pure neural network based on a model submitted to Kaggle (https://www.kaggle.com/tunguz/bi-gru-lstm-dual-embedding-new-test-cleaned-5). The model in the API is not quite that, but it's very similar. Ultimately, we wanted to do some neural architecture search through either Ludwig or NNI. Ludwig's results were disappointing, and we could not get NNI NAS working in time. We were able to do hyperparameter tuning using NNI, but this did not result in significant improvement over the default in the week we spent on it.
 